{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model,model_from_json\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "from qkeras import QDense, QActivation\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow_model_optimization as tfmot\n",
    "tsk = tfmot.sparsity.keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from functions import preprocess_anomaly_data, make_mse_loss,\\\n",
    "roc_objective,load_model, save_model\n",
    "from autoencoder_classes import AE\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = (N,19,3,1).flatten()\n",
    "with open('/eos/user/e/epuljak/forDelphes/Delphes_QCD_BSM_data.pkl', 'rb') as f:\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters for QKeras and Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_size = 0\n",
    "integer = 1\n",
    "pruning = 'pruned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pruning == 'pruned':\n",
    "    begin_step = np.ceil((X_train_flatten.shape[0]*0.8)/1024).astype(np.int32)*5\n",
    "    end_step = np.ceil((X_train_flatten.shape[0]*0.8)/1024).astype(np.int32)*15\n",
    "    print('Begin step: ' + str(begin_step) + ', End step: ' + str(end_step))\n",
    "    pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
    "                            initial_sparsity=0.0, final_sparsity=0.5,\n",
    "                            begin_step=begin_step, end_step=end_step)\n",
    "    print(pruning_schedule.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "input_shape = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "# x = Activation('linear')(inputArray) if quant_size==0\\\n",
    "#     else QActivation(f'quantized_bits(16,10,1, alpha=1.0)')(inputArray)\n",
    "x = BatchNormalization()(inputArray)\n",
    "x = tsk.prune_low_magnitude(Dense(32, kernel_initializer=tf.keras.initializers.HeUniform()),\\\n",
    "                           pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QDense(32, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)'),\\\n",
    "                                 pruning_schedule=pruning_schedule)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tsk.prune_low_magnitude(LeakyReLU(alpha=0.3),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),pruning_schedule=pruning_schedule)(x)\n",
    "x = tsk.prune_low_magnitude(Dense(16, kernel_initializer=tf.keras.initializers.HeUniform()),\\\n",
    "                            pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QDense(16, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)'),\\\n",
    "                                 pruning_schedule=pruning_schedule)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tsk.prune_low_magnitude(LeakyReLU(alpha=0.3),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),pruning_schedule=pruning_schedule)(x)\n",
    "encoder = tsk.prune_low_magnitude(Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform()),\\\n",
    "                            pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(16) + ',' + str(6) + ',1, alpha=1.0)',\\\n",
    "               bias_quantizer='quantized_bits(' + str(16) + ',' + str(6) + ',1, alpha=1.0)'),\\\n",
    "                                 pruning_schedule=pruning_schedule)(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#encoder = tsk.prune_low_magnitude(LeakyReLU(alpha=0.3),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    #else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),pruning_schedule=pruning_schedule)(x)\n",
    "\n",
    "#decoder\n",
    "x = tsk.prune_low_magnitude(Dense(16, kernel_initializer=tf.keras.initializers.HeUniform()),\\\n",
    "                            pruning_schedule=pruning_schedule)(encoder) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QDense(16, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)'),\\\n",
    "                                 pruning_schedule=pruning_schedule)(encoder)\n",
    "x = BatchNormalization()(x)\n",
    "x = tsk.prune_low_magnitude(LeakyReLU(alpha=0.3),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),pruning_schedule=pruning_schedule)(x)\n",
    "x = tsk.prune_low_magnitude(Dense(32, kernel_initializer=tf.keras.initializers.HeUniform()),\\\n",
    "                            pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QDense(32, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)',\\\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)'),\\\n",
    "                                 pruning_schedule=pruning_schedule)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tsk.prune_low_magnitude(LeakyReLU(alpha=0.3),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),pruning_schedule=pruning_schedule)(x)\n",
    "decoder = tsk.prune_low_magnitude(Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(),  name='output_dense'),\\\n",
    "                                  pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "        else tsk.prune_low_magnitude(QDense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(16) + ',' + str(10) + ',1, alpha=1.0)',\\\n",
    "               bias_quantizer='quantized_bits(' + str(16) + ',' + str(10) + ',1, alpha=1.0)'),\\\n",
    "                                  pruning_schedule=pruning_schedule)(x)\n",
    "\n",
    "#create autoencoder\n",
    "autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AE(autoenc=autoencoder)\n",
    "ae.compile(optimizer=keras.optimizers.Adam(lr=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer weights\n",
    "model_dir = 'AE_models/final_models/withCorrectPrefiltering/'\n",
    "name_encoder ='AE_notpruned'\n",
    "baseline_AE = load_model(model_dir+name_encoder)\n",
    "\n",
    "# set weights for encoder\n",
    "for i, l in enumerate(ae.autoencoder.layers):\n",
    "    if i < 1: continue\n",
    "    ae.autoencoder.layers[i].set_weights(baseline_AE.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "\n",
    "callbacks=[]\n",
    "if pruning=='pruned':\n",
    "    callbacks.append(tfmot.sparsity.keras.UpdatePruningStep())\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "#tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best_2.h5'.format(odir),monitor=\"val_loss\",verbose=0,save_best_only=True),\n",
    "#tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best_weights_2.h5'.format(odir),monitor=\"val_loss\",verbose=0,save_weights_only=True),\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING\")\n",
    "history = ae.fit(X_train_flatten, X_train_scaled, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_split=0.2,\n",
    "                  callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_autoencoder = tfmot.sparsity.keras.strip_pruning(ae.autoencoder)\n",
    "final_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('AE_models/final_models/withCorrectPrefiltering/AE_pruned', final_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_autoencoder = load_model('AE_models/final_models/withCorrectPrefiltering/AE_pruned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training/validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'][0:], label='Training loss')\n",
    "plt.plot(history.history['val_loss'][0:], label='Validation loss')\n",
    "plt.title('Training and validation loss - MSE')\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check sparsity of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pruned weights\n",
    "for i, w in enumerate(final_autoencoder.get_weights()):\n",
    "    print(\n",
    "        \"{} -- Total:{}, Zeros: {:.2f}%\".format(\n",
    "            final_autoencoder.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model after training\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(final_model.layers[3].get_weights()[0].reshape((57*32,)), label='Encoder 32', bins=100, alpha=0.5)\n",
    "plt.hist(final_model.layers[6].get_weights()[0].reshape((32*16,)), label='Encoder 16',bins=100, alpha = 0.7)\n",
    "plt.hist(final_model.layers[9].get_weights()[0].reshape((16*3,)), label='latent',bins=100)\n",
    "plt.hist(final_model.layers[10].get_weights()[0].reshape((16*3,)), label='Decoder 16',bins=100, alpha=0.6)\n",
    "plt.hist(final_model.layers[13].get_weights()[0].reshape((32*16,)), label='Decoder 32',bins=100, alpha=0.7)\n",
    "plt.hist(final_model.layers[16].get_weights()[0].reshape((32*57,)), label='Output',bins=100,alpha=0.5)\n",
    "\n",
    "#plt.yscale('log', nonpositive='clip')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Weights')\n",
    "plt.ylabel('Number of Weights')\n",
    "plt.title('Pruned 5 to 15')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction - background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/eos/user/e/epuljak/forDelphes/Delphes_QCD_BSM_data_half1.pkl', 'rb') as f:\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd_prediction = final_autoencoder.predict(X_test_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction - Beyond Standard Model events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_labels = ['Leptoquark','A to 4 leptons', 'hChToTauNu', 'hToTauTau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_results = []\n",
    "\n",
    "for i, label in enumerate(bsm_labels[:]):\n",
    "    bsm_prediction = autoencoder.predict(bsm_data[i])\n",
    "    bsm_results.append([label, bsm_target[i], bsm_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = 'AE_result_pruned.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(output_result, 'w')\n",
    "h5f.create_dataset('QCD', data = X_test_scaled)\n",
    "h5f.create_dataset('QCD_input', data=X_test_flatten)\n",
    "h5f.create_dataset('predicted_QCD', data = qcd_prediction)\n",
    "\n",
    "for i, bsm in enumerate(bsm_results):\n",
    "    h5f.create_dataset('%s_scaled' %bsm[0], data=bsm[1])\n",
    "    h5f.create_dataset('%s_input' %bsm[0], data=bsm_data[i])\n",
    "    h5f.create_dataset('predicted_%s' %bsm[0], data=bsm[2])\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE projections of latent space representations - for QCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "keras_trace = hls4ml.model.profiling.get_ymodel_keras(autoencoder, X_test_flatten[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_traces = []\n",
    "\n",
    "for i, label in enumerate(bsm_labels[:2]):\n",
    "    bsm_trace = hls4ml.model.profiling.get_ymodel_keras(autoencoder, bsm_data[i][:10000])\n",
    "    bsm_traces.append(bsm_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D PROJECTIONS\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "idx_max = 1000\n",
    "#prediction\n",
    "z_dset1 = keras_trace['dense_2'][:1000]\n",
    "z_dset2 = bsm_traces[0]['dense_2'][:idx_max]\n",
    "z_dset3 = bsm_traces[1]['dense_2'][:idx_max]\n",
    "\n",
    "z_embedded1 = TSNE(n_components=2).fit_transform(z_dset1)\n",
    "z_embedded2 = TSNE(n_components=2).fit_transform(z_dset2)\n",
    "z_embedded3 = TSNE(n_components=2).fit_transform(z_dset3)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(z_embedded1[:,0], z_embedded1[:,1],'o', mew=1.2, mfc='none', label='Standard Model', color='indigo')\n",
    "plt.plot(z_embedded2[:,0], z_embedded2[:,1],'s', mew=1.2, mfc='none', label=r'LQ $\\rightarrow$ b$\\tau$', color='forestgreen')\n",
    "plt.plot(z_embedded3[:,0], z_embedded3[:,1],'v', mew=1.2, mfc='none', label=r'A $\\rightarrow$ 4L', color='tomato')\n",
    "\n",
    "plt.xlabel('$z_\\mathrm{1}$')\n",
    "plt.ylabel('$z_\\mathrm{2}$')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('TSNE_AE_pruned_1000.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "neptune": {
   "notebookId": "db31bc2a-7b92-40f5-8253-dfc65381a6fb",
   "projectVersion": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
