{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model,model_from_json\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "from qkeras import QDense, QActivation\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import pathlib\n",
    "#import tensorflow_model_optimization as tfmot\n",
    "#tsk = tfmot.sparsity.keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from functions import preprocess_anomaly_data, custom_loss_negative, custom_loss_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-tobacco",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('Delphes_dataset_HALF.h5', 'r')\n",
    "X_train_flatten = np.array(file['X_train_flatten'])\n",
    "X_test_flatten = np.array(file['X_test_flatten'])\n",
    "X_val_flatten = np.array(file['X_val_flatten'])\n",
    "\n",
    "X_train_scaled = np.array(file['X_train_scaled'])\n",
    "#X_test_scaled = np.array(file['X_test_scaled'])\n",
    "X_val_scaled = np.array(file['X_val_scaled'])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-conference",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "input_shape = 56\n",
    "\n",
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "x = Activation('linear', name='block_1_act')(inputArray)\n",
    " #   else QActivation(f'quantized_bits(16,6,1)')(inputArray)\n",
    "x = BatchNormalization(name='bn_1')(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_2_dense')(x)\n",
    "x = BatchNormalization(name='bn_2')(x)\n",
    "x = Activation('relu', name='block_2_act')(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_3_dense')(x)\n",
    "x = BatchNormalization(name='bn_3')(x)\n",
    "x = Activation('relu', name='block_3_act')(x)\n",
    "encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform(),name='bottleneck')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "\n",
    "#decoder\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_4_dense')(encoder)\n",
    "x = BatchNormalization(name='bn_4')(x)\n",
    "x = Activation('relu', name='block_4_act')(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_5_dense')(x)\n",
    "x = BatchNormalization(name='bn_5')(x)\n",
    "x = Activation('relu', name='block_5_act')(x)\n",
    "x = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(), name='output_dense')(x)\n",
    "decoder = Activation('linear', name='output_act')(x)\n",
    "\n",
    "#create autoencoder\n",
    "autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-omega",
   "metadata": {},
   "source": [
    "### Load signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "ato4l = h5py.File('Ato4l_lepFilter_13TeV.h5', 'r')\n",
    "ato4l = ato4l['Particles'][:]\n",
    "ato4l = ato4l[:,:,:-1]\n",
    "\n",
    "import joblib\n",
    "pT_scaler = joblib.load('pt_scaled_VAE_new.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_ato4l, test_notscaled_ato4l = preprocess_anomaly_data(pT_scaler, ato4l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-fashion",
   "metadata": {},
   "source": [
    "### Set objective and  compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_data = test_notscaled_ato4l #input - data without any preprocessing\n",
    "autoencoder.compile(optimizer=keras.optimizers.Adam(), loss=custom_loss_training, run_eagerly=True) # just to make sure it runs in eager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-cover",
   "metadata": {},
   "source": [
    "### Override AutoQKeras classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Custom_AutoQKeras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-commerce",
   "metadata": {},
   "source": [
    "### Set AutoQKeras parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "from qkeras.utils import model_quantize\n",
    "from qkeras.qtools import run_qtools\n",
    "from qkeras.qtools import settings as qtools_settings\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "for d in physical_devices:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_internal = \"fp32\"\n",
    "reference_accumulator = \"fp32\"\n",
    "\n",
    "q = run_qtools.QTools(\n",
    "  autoencoder,\n",
    "  # energy calculation using a given process\n",
    "  # \"horowitz\" refers to 45nm process published at\n",
    "  # M. Horowitz, \"1.1 Computing's energy problem (and what we can do about\n",
    "  # it), \"2014 IEEE International Solid-State Circuits Conference Digest of\n",
    "  # Technical Papers (ISSCC), San Francisco, CA, 2014, pp. 10-14, \n",
    "  # doi: 10.1109/ISSCC.2014.6757323.\n",
    "  process=\"horowitz\",\n",
    "  # quantizers for model input\n",
    "  source_quantizers=[quantized_bits(16, 6, 1)],\n",
    "  is_inference=False,\n",
    "  # absolute path (including filename) of the model weights\n",
    "  # in the future, we will attempt to optimize the power model\n",
    "  # by using weight information, although it can be used to further\n",
    "  # optimize QBatchNormalization.\n",
    "  weights_path=None,\n",
    "  # keras_quantizer to quantize weight/bias in un-quantized keras layers\n",
    "  keras_quantizer=reference_internal,\n",
    "  # keras_quantizer to quantize MAC in un-quantized keras layers\n",
    "  keras_accumulator=reference_accumulator,\n",
    "  # whether calculate baseline energy\n",
    "  for_reference=True)\n",
    "  \n",
    "# caculate energy of the derived data type map.\n",
    "energy_dict = q.pe(\n",
    "    # whether to store parameters in dram, sram, or fixed\n",
    "    weights_on_memory=\"sram\",\n",
    "    # store activations in dram or sram\n",
    "    activations_on_memory=\"sram\",\n",
    "    # minimum sram size in number of bits. Let's assume a 16MB SRAM.\n",
    "    min_sram_size=8*16*1024*1024,\n",
    "    rd_wr_on_io=False)\n",
    "\n",
    "# get stats of energy distribution in each layer\n",
    "energy_profile = q.extract_energy_profile(\n",
    "    qtools_settings.cfg.include_energy, energy_dict)\n",
    "# extract sum of energy of each layer according to the rule specified in\n",
    "# qtools_settings.cfg.include_energy\n",
    "total_energy = q.extract_energy_sum(\n",
    "    qtools_settings.cfg.include_energy, energy_dict)\n",
    "\n",
    "pprint.pprint(energy_profile)\n",
    "print()\n",
    "print(\"Total energy: {:.4f} uJ\".format(total_energy / 1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = {\n",
    "        \"kernel\": {\n",
    "                \"quantized_bits(2,1,1,alpha=1.0)\": 2,\n",
    "                \"quantized_bits(4,2,1,alpha=1.0)\": 4,\n",
    "                \"quantized_bits(6,2,1,alpha=1.0)\": 6,\n",
    "                \"quantized_bits(8,3,1,alpha=1.0)\": 8,\n",
    "                \"quantized_bits(10,3,1,alpha=1.0)\": 10,\n",
    "                \"quantized_bits(12,4,1,alpha=1.0)\": 12,\n",
    "                \"quantized_bits(14,4,1,alpha=1.0)\": 14,\n",
    "                \"quantized_bits(16,6,1,alpha=1.0)\": 16\n",
    "        },\n",
    "        \"bias\": {\n",
    "                \"quantized_bits(2,1,1)\": 2,\n",
    "                \"quantized_bits(4,2,1)\": 4,\n",
    "                \"quantized_bits(6,2,1)\": 6,\n",
    "                \"quantized_bits(8,3,1)\": 8\n",
    "        },\n",
    "        \"activation\": {\n",
    "                \"quantized_relu(2,1)\": 2,\n",
    "                \"quantized_relu(3,1)\": 3,\n",
    "                \"quantized_relu(4,2)\": 4,\n",
    "                \"quantized_relu(6,2)\": 6,\n",
    "                \"quantized_relu(8,3)\": 8,\n",
    "                \"quantized_relu(10,3)\": 10,\n",
    "                \"quantized_relu(12,4)\": 12,\n",
    "                \"quantized_relu(14,4)\": 14,\n",
    "                \"quantized_relu(16,6)\": 16\n",
    "        },\n",
    "        \"linear\": {\n",
    "                \"quantized_bits(16,6)\": 16\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = {\n",
    "    \"Dense\": [16, 8, 16],\n",
    "    \"Activation\": [16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = {\n",
    "    \"type\": \"energy\",\n",
    "    \"params\": {\n",
    "        \"delta_p\": 8.0,\n",
    "        \"delta_n\": 8.0,\n",
    "        \"rate\": 4.0, # a try\n",
    "        \"stress\": 0.7, # a try\n",
    "        \"process\": \"horowitz\",\n",
    "        \"parameters_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"activations_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"rd_wr_on_io\": [False, False],\n",
    "        \"min_sram_size\": [0, 0],\n",
    "        \"source_quantizers\": [\"fp16\"],\n",
    "        \"reference_internal\": \"fp16\",\n",
    "        \"reference_accumulator\": \"fp16\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "odir='autoqkeras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    \"output_dir\": \"{}/\".format(odir),\n",
    "    \"goal\": goal,\n",
    "    \"quantization_config\": quantization_config,\n",
    "    \"learning_rate_optimizer\": False,\n",
    "    \"transfer_weights\": False,\n",
    "    \"mode\": \"bayesian\",\n",
    "    #\"score_metric\": \"val_roc_objective_val\",\n",
    "    \"seed\": 42,\n",
    "    \"limit\": limit,\n",
    "    \"tune_filters\": \"layer\",\n",
    "    \"tune_filters_exceptions\": \"^output.*\",\n",
    "    \"layer_indexes\": [1,3,5,6,8,9,10,12,13,15,16,17],\n",
    "    \"max_trials\": 130,\n",
    "    \"blocks\": [\n",
    "          \"block_1_.*$\",\n",
    "          \"block_2_.*$\",\n",
    "          \"block_3_.*$\",\n",
    "          \"bottleneck$\",\n",
    "          \"block_4_.*$\",\n",
    "          \"block_5_.*$\",\n",
    "          \"output_dense$\",\n",
    "          \"output_act$\",],\n",
    "    \"schedule_block\": \"cost\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"quantizing layers:\", [autoencoder.layers[i].name for i in run_config[\"layer_indexes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "\n",
    "callbacks=[]\n",
    "#if pruning=='pruned':\n",
    " #   callbacks.append(tfmot.sparsity.keras.UpdatePruningStep())\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "#callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best.h5'.format(odir),monitor=\"val_loss\",verbose=0,save_best_only=True))\n",
    "callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best_weights.h5'.format(odir),monitor=\"val_loss\",verbose=0,save_weights_only=True))\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-carry",
   "metadata": {},
   "source": [
    "### Run search with AutoQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoqk = Custom_AutoQKerasScheduler(autoencoder,metrics=[custom_loss_negative], X_test = X_test_flatten[:3000000], bsm_data = bsm_data,\\\n",
    "                             custom_objects={}, debug=False, **run_config)\n",
    "autoqk.fit(X_train_flatten[:3000000], X_train_scaled[:3000000],\\\n",
    "           validation_data=(X_val_flatten[:3000000], X_val_scaled[:3000000]),\\\n",
    "           batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = autoqk.get_best_model()\n",
    "qmodel.summary()\n",
    "save_model('best_pretrain_objective_roc', qmodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
