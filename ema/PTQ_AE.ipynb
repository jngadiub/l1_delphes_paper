{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import setGPU\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, Layer, Reshape\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from qkeras import QDense, QActivation\n",
    "from qkeras import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from functions import preprocess_anomaly_data, custom_loss_negative, custom_loss_training,\\\n",
    "roc_objective,load_model, save_model\n",
    "from custom_layers import Sampling\n",
    "\n",
    "from autoencoder_classes import AE\n",
    "\n",
    "tsk = tfmot.sparsity.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = (N,19,3,1).flatten()\n",
    "with open('/eos/user/e/epuljak/forDelphes/Delphes_QCD_BSM_data.pkl', 'rb') as f:\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_size = 16\n",
    "integer = 6\n",
    "symmetric = 1\n",
    "alpha=1\n",
    "pruning='pruned'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-brake",
   "metadata": {},
   "source": [
    "### Define model\n",
    "Prune and quantize only encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "input_shape = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "x = Activation('linear')(inputArray) if quant_size==0\\\n",
    "   else QActivation(f'quantized_bits(16,10,1, alpha=1.0)')(inputArray)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x) if quant_size==0\\\n",
    "    else QDense(32, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x) if quant_size==0\\\n",
    "    else QActivation('quantized_relu(bits=' + str(quant_size) + ')')(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(x) if quant_size==0\\\n",
    "    else QDense(16, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x) if quant_size==0\\\n",
    "    else QActivation('quantized_relu(bits=' + str(quant_size) + ')')(x)\n",
    "encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x) if quant_size==0\\\n",
    "    else QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)',\\\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#encoder = tsk.prune_low_magnitude(LeakyReLU(alpha=0.3),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    #else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),pruning_schedule=pruning_schedule)(x)\n",
    "\n",
    "#decoder\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(encoder) if quant_size==0\\\n",
    "    else QDense(16, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)',\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)')(encoder)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x) if quant_size==0\\\n",
    "    else QActivation('quantized_relu(bits=' + str(quant_size) + ')')(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x) if quant_size==0\\\n",
    "    else QDense(32, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)',\\\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ',' + str(integer) + ',1, alpha=1.0)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x) if quant_size==0\\\n",
    "    else QActivation('quantized_relu(bits=' + str(quant_size) + ')')(x)\n",
    "decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(),  name='output_dense')(x) if quant_size==0\\\n",
    "        else QDense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(16) + ',' + str(10) + ',1, alpha=1.0)',\\\n",
    "               bias_quantizer='quantized_bits(' + str(16) + ',' + str(10) + ',1, alpha=1.0)')(x)\n",
    "\n",
    "#create autoencoder\n",
    "autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AE(autoenc=autoencoder)\n",
    "ae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer weights\n",
    "model_dir = 'AE_models/final_models/withCorrectPrefiltering/'\n",
    "name_encoder ='AE_pruned'\n",
    "baseline_AE = load_model(model_dir+name_encoder, custom_objects={'QDense': QDense, 'QActivation': QActivation})\n",
    "\n",
    "# set weights for encoder\n",
    "for i, l in enumerate(ae.autoencoder.layers):\n",
    "    if i < 2: continue\n",
    "    ae.autoencoder.layers[i].set_weights(baseline_AE.layers[i-1].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-electricity",
   "metadata": {},
   "source": [
    "## Add MSE layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers import CustomMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = ae.autoencoder.input\n",
    "predicted = ae.autoencoder.layers[-1].output\n",
    "reshaped_predicted = Reshape((19,3,1), name='reshaped_predicted')(predicted)\n",
    "\n",
    "scaled_input = BatchNormalization(trainable=False, name='scaled_input')(true)\n",
    "reshaped_scaled_input = Reshape((19,3,1), name='reshaped_scaled_input')(scaled_input)\n",
    "# calculate MSE between them\n",
    "custom_output = CustomMSE()([reshaped_scaled_input, reshaped_predicted])\n",
    "# create new model\n",
    "model = Model(inputs=ae.autoencoder.input, outputs=custom_output)\n",
    "\"\"\" Keras BatchNorm layer returns\n",
    "    gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta\n",
    "    epsilon=0.001\n",
    "    momentum=0.99\n",
    "    moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)\n",
    "    moving_variance = moving_var * momentum + var(batch) * (1 - momentum)\n",
    "    pt_scaler\n",
    "    pt_scaler.mean_\n",
    "    pt_scaler.var_\n",
    "\"\"\"\n",
    "# with open('output/data_-1.pickle', 'rb') as f:\n",
    "#     x_train, y_train, _, _, _, pt_scaler = pickle.load(f)\n",
    "mean_ = np.zeros((57,))\n",
    "var_ = np.ones((57,))\n",
    "for i in range(19):\n",
    "    mean_[3*i] = pt_scaler.mean_[i]\n",
    "    var_[3*i] = pt_scaler.var_[i]\n",
    "# order of weights is (gamma,beta,mean,std)\n",
    "model.get_layer('scaled_input').set_weights((np.ones((57,)),np.zeros((57,)),mean_,var_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('AE_models/final_models/withCorrectPrefiltering/PTQ_AE_qkeras14', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-society",
   "metadata": {},
   "source": [
    "## Compare QKeras model and BP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/eos/user/e/epuljak/forDelphes/Delphes_QCD_BSM_data_half2.pkl', 'rb') as f:\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom model for BP model\n",
    "\n",
    "true_BP = baseline_AE.input\n",
    "predicted_BP = baseline_AE.layers[-1].output\n",
    "reshaped_predicted_BP = Reshape((19,3,1), name='reshaped_predicted_bp')(predicted_BP)\n",
    "\n",
    "scaled_input_BP = BatchNormalization(trainable=False, name='scaled_input_bp')(true_BP)\n",
    "reshaped_scaled_input_BP = Reshape((19,3,1), name='reshaped_scaled_input_bp')(scaled_input_BP)\n",
    "# calculate MSE between them\n",
    "custom_output_BP = CustomMSE()([reshaped_scaled_input_BP, reshaped_predicted_BP])\n",
    "# create new model\n",
    "model_BP = Model(inputs=baseline_AE.input, outputs=custom_output_BP)\n",
    "\"\"\" Keras BatchNorm layer returns\n",
    "    gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta\n",
    "    epsilon=0.001\n",
    "    momentum=0.99\n",
    "    moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)\n",
    "    moving_variance = moving_var * momentum + var(batch) * (1 - momentum)\n",
    "    pt_scaler\n",
    "    pt_scaler.mean_\n",
    "    pt_scaler.var_\n",
    "\"\"\"\n",
    "# with open('output/data_-1.pickle', 'rb') as f:\n",
    "#     x_train, y_train, _, _, _, pt_scaler = pickle.load(f)\n",
    "mean_BP = np.zeros((57,))\n",
    "var_BP = np.ones((57,))\n",
    "for i in range(19):\n",
    "    mean_BP[3*i] = pt_scaler.mean_[i]\n",
    "    var_BP[3*i] = pt_scaler.var_[i]\n",
    "# order of weights is (gamma,beta,mean,std)\n",
    "model_BP.get_layer('scaled_input_bp').set_weights((np.ones((57,)),np.zeros((57,)),mean_BP,var_BP))\n",
    "model_BP.summary()\n",
    "model_BP.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_qkeras = ae.autoencoder.predict(X_test_flatten)\n",
    "y_BP = baseline_AE.predict(X_test_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import make_mse_loss_numpy\n",
    "mse_loss_total = []\n",
    "mse_loss_total.append(make_mse_loss_numpy(X_test_flatten,y_qkeras))\n",
    "mse_loss_total.append(make_mse_loss_numpy(X_test_flatten,y_BP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_labels = ['Leptoquark','A to 4 leptons', 'hChToTauNu', 'hToTauTau']\n",
    "labels = ['QCD QKeras', 'QCD BP Keras',\\\n",
    "          r'QKeras LQ $\\rightarrow$ b$\\tau$', r'BP Keras LQ $\\rightarrow$ b$\\tau$',\\\n",
    "          r'QKeras A $\\rightarrow$ 4L', r'BP Keras A $\\rightarrow$ 4L',\\\n",
    "          r'QKeras $h_{\\pm} \\rightarrow \\tau\\nu$', r'BP Keras $h_{\\pm} \\rightarrow \\tau\\nu$',\\\n",
    "          r'QKeras $h_{0} \\rightarrow \\tau\\tau$', r'BP Keras $h_{0} \\rightarrow \\tau\\tau$']\n",
    "loss = '$MSE$'\n",
    "\n",
    "colors = ['C1','C2', 'C3', 'C4', 'C5', 'C6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(bsm_labels):\n",
    "    qkeras_pred = ae.autoencoder.predict(bsm_data[i])\n",
    "    BP_pred = baseline_AE.predict(bsm_data[i])\n",
    "    \n",
    "    mse_loss_total.append(make_mse_loss_numpy(bsm_data[i],qkeras_pred))\n",
    "    mse_loss_total.append(make_mse_loss_numpy(bsm_data[i],BP_pred))\n",
    "    print(\"========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "minScore = 999999.\n",
    "maxScore = 0\n",
    "for i in range(len(labels)):\n",
    "    thisMin = np.min(mse_loss_total[i])\n",
    "    thisMax = np.max(mse_loss_total[i])\n",
    "    minScore = min(thisMin, minScore)\n",
    "    maxScore = max(maxScore, thisMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss distributions\n",
    "bin_size=100\n",
    "plt.figure(figsize=(10,8))\n",
    "z = 0\n",
    "for i, label in enumerate(labels):\n",
    "    if i%2==0:\n",
    "        plt.hist(mse_loss_total[i+2].reshape(mse_loss_total[i+2].shape[0]*1), bins=bin_size, label=label, density = True, range=(minScore, 10),\n",
    "         histtype='step', fill=False, linewidth=1.5, color=colors[z])\n",
    "    if i%2==1:\n",
    "        plt.hist(mse_loss_total[i+2].reshape(mse_loss_total[i+2].shape[0]*1), bins=bin_size, label=label, density = True, range=(minScore, 10),\n",
    "         histtype='step', fill=False, linewidth=1.5, alpha=0.6, color=colors[z])\n",
    "        z = z+1\n",
    "#plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Probability (a.u.)\")\n",
    "plt.grid(True)\n",
    "plt.title('KL loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data for ROCs\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "tpr_lq=[];fpr_lq=[];auc_lq=[]\n",
    "tpr_ato4l=[];fpr_ato4l=[];auc_ato4l=[]\n",
    "tpr_ch=[];fpr_ch=[];auc_ch=[]\n",
    "tpr_to=[];fpr_to=[];auc_to=[]\n",
    "\n",
    "\n",
    "target_qcd_qkeras = np.zeros(mse_loss_total[0].shape[0])\n",
    "target_qcd_BP = np.zeros(mse_loss_total[1].shape[0])\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if i == 0 and i==1: continue\n",
    "    if i%2==0:\n",
    "        trueVal = np.concatenate((np.ones(mse_loss_total[i].shape[0]), target_qcd_qkeras))\n",
    "        predVal_loss = np.concatenate((mse_loss_total[i], mse_loss_total[0]))\n",
    "\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "        if i==2:\n",
    "            tpr_lq.append(tpr_loss)\n",
    "            fpr_lq.append(fpr_loss)\n",
    "            auc_lq.append(auc_loss)\n",
    "        elif i == 4:\n",
    "            tpr_ato4l.append(tpr_loss)\n",
    "            fpr_ato4l.append(fpr_loss)\n",
    "            auc_ato4l.append(auc_loss)\n",
    "        elif i==6:\n",
    "            tpr_ch.append(tpr_loss)\n",
    "            fpr_ch.append(fpr_loss)\n",
    "            auc_ch.append(auc_loss)\n",
    "        elif i == 8:\n",
    "            tpr_to.append(tpr_loss)\n",
    "            fpr_to.append(fpr_loss)\n",
    "            auc_to.append(auc_loss)\n",
    "    if i%2==1:\n",
    "        \n",
    "        trueVal = np.concatenate((np.ones(mse_loss_total[i].shape[0]), target_qcd_BP))\n",
    "        predVal_loss = np.concatenate((mse_loss_total[i], mse_loss_total[1]))\n",
    "\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "        if i==3:\n",
    "            tpr_lq.append(tpr_loss)\n",
    "            fpr_lq.append(fpr_loss)\n",
    "            auc_lq.append(auc_loss)\n",
    "        elif i == 5:\n",
    "            tpr_ato4l.append(tpr_loss)\n",
    "            fpr_ato4l.append(fpr_loss)\n",
    "            auc_ato4l.append(auc_loss)\n",
    "        elif i==7:\n",
    "            tpr_ch.append(tpr_loss)\n",
    "            fpr_ch.append(fpr_loss)\n",
    "            auc_ch.append(auc_loss)\n",
    "        elif i == 9:\n",
    "            tpr_to.append(tpr_loss)\n",
    "            fpr_to.append(fpr_loss)\n",
    "            auc_to.append(auc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ROCs\n",
    "plt.figure(figsize=(12,8))\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_lq[:], fpr_lq[:], auc_lq[:], labels[2:4])):\n",
    "    if i == 1:\n",
    "        plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[0], alpha=0.6)\n",
    "    else: \n",
    "        plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[0])\n",
    "\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_ato4l[:], fpr_ato4l[:], auc_ato4l[:], labels[4:6])):\n",
    "    if i == 1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[1], alpha = 0.6)\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[1])\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_ch[:], fpr_ch[:], auc_ch[:], labels[6:8])):\n",
    "    if i==1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[2], alpha=0.6)\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[2])\n",
    "\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_to[:], fpr_to[:], auc_to[:], labels[8:])):\n",
    "    if i==1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[3], alpha=0.6)\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[3])\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True)\n",
    "#plt.ylim(0,0.5)\n",
    "plt.legend(bbox_to_anchor=[1.2, 0.5],loc='best',frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='red', linestyle='dashed', linewidth=1)\n",
    "#plt.title('QKERAS <16,6>')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = '/eos/user/e/epuljak/forDelphes/CorrectDataResults/PTQ_AE_result_qkeras14.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(output_result, 'w')\n",
    "h5f.create_dataset('QCD_Qkeras', data = mse_loss_total[0])\n",
    "h5f.create_dataset('QCD_BP', data = mse_loss_total[1])\n",
    "for i,bsm in enumerate(bsm_labels[:]):\n",
    "    print(i)\n",
    "    if i == 0: z = 2\n",
    "    elif i == 1: z = 4\n",
    "    elif i == 2: z = 6\n",
    "    elif i == 3: z = 8\n",
    "    h5f.create_dataset('%s_Qkeras' %bsm, data = mse_loss_total[z])\n",
    "    h5f.create_dataset('%s_BP'%bsm, data = mse_loss_total[z+1])\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-orlando",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
