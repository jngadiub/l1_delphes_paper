{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import setGPU\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "\n",
    "from functions import preprocess_anomaly_data, custom_loss_negative, custom_loss_training,\\\n",
    "roc_objective,load_model, save_model\n",
    "from custom_layers import Sampling\n",
    "from autoencoder_classes import VAE\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = (N,19,3,1).flatten()\n",
    "with open('/eos/user/e/epuljak/forDelphes/Delphes_QCD_BSM_data.pkl', 'rb') as f:\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "input_shape = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "x = BatchNormalization()(inputArray)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "mu = Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "logvar = Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "# Use reparameterization trick to ensure correct gradient\n",
    "z = Sampling()([mu, logvar])\n",
    "\n",
    "# Create encoder\n",
    "encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "#decoder\n",
    "d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(d_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "# Create decoder\n",
    "decoder = Model(d_input, dec, name='decoder')\n",
    "decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, update_freq='batch', write_images=True, profile_batch=0, histogram_freq=0)\n",
    "#tensorboard_callback.set_model(vae)\n",
    "\n",
    "# tensorboard = False\n",
    "# if tensorboard:\n",
    "#     tracking_address = os.path.join(os.getcwd(), \"tracking_dir\")\n",
    "#     tb = program.TensorBoard()\n",
    "#     tb.configure(argv=[None, '--logdir', tracking_address])\n",
    "#     url = tb.launch()\n",
    "\n",
    "#     if not os.path.exists(tracking_address):\n",
    "#         os.makedirs(tracking_address)\n",
    "\n",
    "#     now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "#     model_name = 'VAE_SGD'\n",
    "\n",
    "#     exp_dir = os.path.join(tracking_address, model_name + '_' + str(now))\n",
    "#     if not os.path.exists(exp_dir):\n",
    "#         os.makedirs(exp_dir)\n",
    "\n",
    "#     tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "#     if not os.path.exists(tb_dir):\n",
    "#         os.makedirs(tb_dir)\n",
    "\n",
    "#     # By default shows losses and metrics for both training and validation\n",
    "#     tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "#                                               profile_batch=0,\n",
    "#                                               histogram_freq=1)  # if 1 shows weights histograms\n",
    "#     callbacks.append(tb_callback)\n",
    "#     #%load_ext tensorboard\n",
    "#     #%tensorboard --logdir /tracking_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024\n",
    "VALIDATION_SPLIT = 0.3\n",
    "#METRICS = [kl_loss, reco_loss]\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "callbacks=[]\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "#callbacks.append(NeptuneMonitor())\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING\")\n",
    "history = vae.fit(X_train_flatten, X_train_scaled, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_split=0.3,\n",
    "                  callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "vae.save('Vae_Delphes_seedDense_beta70')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_enc, model_dec = VAE.load('Vae_Delphes_meanKL_SGD', custom_objects={'Sampling': Sampling})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training/validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'][1:], label='Training loss')\n",
    "plt.plot(history.history['val_loss'][1:], label='Validation loss')\n",
    "plt.title('Training and validation loss - MAE')\n",
    "plt.yscale('log', nonpositive='clip')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['kl_loss'][1:], label='Training loss')\n",
    "plt.plot(history.history['val_kl_loss'][1:], label='Validation loss')\n",
    "plt.title('Training and validation KL loss')\n",
    "plt.yscale('log', nonpositive='clip')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model('VAE_models/final_models/withCorrectPrefiltering/VAE_encoder_notpruned', vae.encoder)\n",
    "# save_model('VAE_models/final_models/withCorrectPrefiltering/VAE_decoder_notpruned', vae.decoder)\n",
    "\n",
    "# encoder = load_model('VAE_models/final_models/withCorrectPrefiltering/VAE_encoder_notpruned', custom_objects={'Sampling': Sampling})\n",
    "# decoder = load_model('VAE_models/final_models/withCorrectPrefiltering/VAE_decoder_notpruned', custom_objects={'Sampling': Sampling})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_labels = ['Leptoquark','A to 4 leptons', 'hChToTauNu', 'hToTauTau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_results = []\n",
    "\n",
    "for i, label in enumerate(bsm_labels):\n",
    "    mean_pred, logvar_pred, z_pred = encoder(bsm_data[i])\n",
    "    bsm_prediction = decoder(z_pred)\n",
    "    print(bsm_prediction.shape)\n",
    "    bsm_results.append([label, bsm_target[i], bsm_prediction, mean_pred, logvar_pred, z_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = 'VAE_result_notpruned_alldata.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(output_result, 'w')\n",
    "h5f.create_dataset('QCD', data = X_test_scaled)\n",
    "h5f.create_dataset('QCD_input', data=X_test_flatten)\n",
    "h5f.create_dataset('predicted_QCD', data = qcd_prediction)\n",
    "h5f.create_dataset('encoded_mean_QCD', data = qcd_mean)\n",
    "h5f.create_dataset('encoded_logvar_QCD', data = qcd_logvar)\n",
    "h5f.create_dataset('encoded_z_QCD', data = qcd_z)\n",
    "for i, bsm in enumerate(bsm_results):\n",
    "    h5f.create_dataset('%s_scaled' %bsm[0], data=bsm[1])\n",
    "    h5f.create_dataset('%s_input' %bsm[0], data=bsm_data[i])\n",
    "    h5f.create_dataset('predicted_%s' %bsm[0], data=bsm[2])\n",
    "    h5f.create_dataset('encoded_mean_%s' %bsm[0], data=bsm[3])\n",
    "    h5f.create_dataset('encoded_logvar_%s' %bsm[0], data=bsm[4])\n",
    "    h5f.create_dataset('encoded_z_%s' %bsm[0], data=bsm[5])\n",
    "\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "neptune": {
   "notebookId": "db31bc2a-7b92-40f5-8253-dfc65381a6fb",
   "projectVersion": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
