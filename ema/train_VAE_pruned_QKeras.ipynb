{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import setGPU\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "import math\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from qkeras import QDense, QActivation, QBatchNormalization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from functions import preprocess_anomaly_data, custom_loss_negative, custom_loss_training,\\\n",
    "roc_objective,load_model, save_model\n",
    "from custom_layers import Sampling\n",
    "from autoencoder_classes import VAE\n",
    "\n",
    "tsk = tfmot.sparsity.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = (N,19,3,1).flatten()\n",
    "with open('/eos/user/e/epuljak/forDelphes/Delphes_QCD_BSM_data.pkl', 'rb') as f:\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters for QKeras and Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_size = 12\n",
    "integer = 4\n",
    "symmetric = 0\n",
    "pruning='pruned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pruning == 'pruned':\n",
    "    ''' How to estimate the enc step:\n",
    "            num_samples = input_train.shape[0] * (1 - validation_split)\n",
    "            end_step = np.ceil(num_samples / batch_size).astype(np.int32) * pruning_epochs\n",
    "            so, stop pruning at the 7th epoch\n",
    "    '''\n",
    "    begin_step = np.ceil((X_train_flatten.shape[0]*0.8)/1024).astype(np.int32)*5\n",
    "    end_step = np.ceil((X_train_flatten.shape[0]*0.8)/1024).astype(np.int32)*15\n",
    "    print('Begin step: ' + str(begin_step) + ', End step: ' + str(end_step))\n",
    "    \n",
    "    pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
    "                            initial_sparsity=0.0, final_sparsity=0.5,\n",
    "                            begin_step=begin_step, end_step=end_step)\n",
    "    print(pruning_schedule.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "Prune and quantize only encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "input_shape = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "#proba\n",
    "x = QActivation(f'quantized_bits(16,10,0,alpha=1)')(inputArray)\n",
    "x = QBatchNormalization()(x)\n",
    "x = tsk.prune_low_magnitude(Dense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                             pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QDense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ','+str(integer)+','+ str(symmetric) +'), alpha=1',\\\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ','+ str(integer) + ',' + str(symmetric) +', alpha=1)'),\\\n",
    "                                             pruning_schedule=pruning_schedule)(x)\n",
    "x = tsk.prune_low_magnitude(QBatchNormalization(), pruning_schedule=pruning_schedule)(x)\n",
    "x = tsk.prune_low_magnitude(Activation('relu'),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),pruning_schedule=pruning_schedule)(x)\n",
    "x = tsk.prune_low_magnitude(Dense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                        pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QDense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(quant_size) + ','+str(integer)+','+ str(symmetric) +', alpha=1)',\\\n",
    "               bias_quantizer='quantized_bits(' + str(quant_size) + ','+ str(integer) + ',' + str(symmetric) +', alpha=1)'),\\\n",
    "                                 pruning_schedule=pruning_schedule)(x)\n",
    "x = tsk.prune_low_magnitude(QBatchNormalization(), pruning_schedule=pruning_schedule)(x)\n",
    "x = tsk.prune_low_magnitude(Activation('relu'),pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QActivation('quantized_relu(bits=' + str(quant_size) + ')'),\\\n",
    "                                 pruning_schedule=pruning_schedule)(x)\n",
    "mu = tsk.prune_low_magnitude(Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeNormal(seed=42)))(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)',\\\n",
    "               bias_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)'),\\\n",
    "                                 pruning_schedule=pruning_schedule)(x)\n",
    "logvar = tsk.prune_low_magnitude(Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeNormal(seed=42)),\\\n",
    "                                 pruning_schedule=pruning_schedule)(x) if quant_size==0\\\n",
    "    else tsk.prune_low_magnitude(QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "               kernel_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)',\\\n",
    "               bias_quantizer='quantized_bits(' + str(16) + ',6,'+ str(symmetric) +', alpha=1)'),\\\n",
    "                                 pruning_schedule=pruning_schedule)(x)\n",
    "# Use reparameterization trick to ensure correct gradient\n",
    "z = Sampling()([mu, logvar])\n",
    "\n",
    "# Create encoder\n",
    "encoder = Model(inputArray, [mu, logvar, z], name='encoder')    \n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "#decoder\n",
    "d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(d_input)\n",
    "x = BatchNormalization()(x)\n",
    "#x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x)    \n",
    "x = BatchNormalization()(x)\n",
    "#x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Activation('relu')(x)\n",
    "dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x)\n",
    "# Create decoder\n",
    "decoder = Model(d_input, dec, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BP or higher bit width model\n",
    "model_dir = 'VAE_models/final_models/withCorrectPrefiltering/'\n",
    "name_encoder ='VAE_encoder_qkeras14_QBN_pw_higher'\n",
    "name_decoder ='VAE_decoder_qkeras14_QBN_pw_higher'\n",
    "# , 'QDense': QDense, 'QActivation': QActivation, 'QBatchNormalization': QBatchNormalization\n",
    "custom_objects={'Sampling': Sampling, 'QDense': QDense, 'QActivation': QActivation, 'QBatchNormalization': QBatchNormalization}\n",
    "\n",
    "BP_encoder = load_model(model_dir+name_encoder, custom_objects)\n",
    "BP_decoder = load_model(model_dir+name_decoder, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set weights for encoder\n",
    "for i, l in enumerate(vae.encoder.layers):\n",
    "    if i == 0: continue\n",
    "    vae.encoder.layers[i].set_weights(BP_encoder.layers[i-1].get_weights()) # i-1 because of QActivation layer (remove when loading from qkeras model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check weights\n",
    "# for i in range(0,12):\n",
    "#     if i < 2: continue\n",
    "#     print('QModel layer: '+str(vae.encoder.layers[i])+', BP model layer: '+str(BP_encoder.layers[i-1]))\n",
    "#     for w1, w2 in zip(vae.encoder.layers[i].get_weights(), BP_encoder.layers[i-1].get_weights()):\n",
    "#         if len(w1) > 1 and len(w2) > 1:\n",
    "#             for weight1, weight2 in zip(w1, w2):\n",
    "#                 print(np.array_equal(weight1, weight2))\n",
    "#         else:\n",
    "#             print(np.array_equal(vae.encoder.layers[i].get_weights(), BP_encoder.layers[i-1].get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set weights for encoder\n",
    "#vae.decoder.load_weights('/eos/user/e/epuljak/autoencoder_models/VAE_decoder_pruned.h5')\n",
    "for i, l in enumerate(vae.decoder.layers):\n",
    "    if i == 0: continue\n",
    "    vae.decoder.layers[i].set_weights(BP_decoder.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check weights\n",
    "# for i, layer in enumerate(vae.decoder.layers):\n",
    "#     if i == 0: continue\n",
    "#     print(' New Model layer: '+str(vae.decoder.layers[i])+', BP model layer: '+str(BP_decoder.layers[i]))\n",
    "#     for w1, w2 in zip(vae.decoder.layers[i].get_weights(), BP_decoder.layers[i].get_weights()):\n",
    "#         print(np.array_equal(w1,w2))\n",
    "#         if len(w1) > 1 and len(w2) > 1:\n",
    "#             for weight1, weight2 in zip(w1, w2):\n",
    "#                 print(np.array_equal(weight1, weight2))\n",
    "#         else:\n",
    "#             print(np.array_equal(vae.decoder.layers[i].get_weights(), BP_decoder.layers[i].get_weights()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[]\n",
    "if pruning=='pruned':\n",
    "    callbacks.append(tfmot.sparsity.keras.UpdatePruningStep())\n",
    "    #callbacks.append(tfmot.sparsity.keras.PruningSummaries(log_dir='vae_prunning'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING\")\n",
    "history = vae.fit(X_train_flatten, X_train_scaled, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_split=0.2,\n",
    "                  callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_encoder = tfmot.sparsity.keras.strip_pruning(vae.encoder)\n",
    "final_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decoder = tfmot.sparsity.keras.strip_pruning(vae.decoder)\n",
    "final_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('VAE_models/final_models/withCorrectPrefiltering/VAE_encoder_qkeras12_QBN_pw_higher', final_encoder)\n",
    "save_model('VAE_models/final_models/withCorrectPrefiltering/VAE_decoder_qkeras12_QBN_pw_higher', final_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_encoder = load_model('VAE_models/final_models/withCorrectPrefiltering/VAE_encoder_pruned', custom_objects={'Sampling': Sampling, 'QDense': QDense, 'QActivation': QActivation})\n",
    "# final_decoder = load_model('VAE_models/final_models/withCorrectPrefiltering/VAE_decoder_pruned', custom_objects={'Sampling': Sampling})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check quantizers\n",
    "for layer in final_encoder.layers:\n",
    "    if hasattr(layer, \"kernel_quantizer\"):\n",
    "        print(layer.name, \"kernel:\", str(layer.kernel_quantizer_internal), \"bias:\", str(layer.bias_quantizer_internal))\n",
    "    elif hasattr(layer, \"quantizer\"):\n",
    "        print(layer.name, \"quantizer:\", str(layer.quantizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check sparsity of weights - encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pruned weights\n",
    "for i, w in enumerate(final_encoder.get_weights()):\n",
    "    print(\n",
    "        \"{} -- Total:{}, Zeros: {:.2f}%\".format(\n",
    "            final_encoder.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors   = ['#a6bddb','#67a9cf','#3690c0','#02818a','#016c59','#014636']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.hist(final_encoder.layers[2].get_weights()[0].reshape((57*32,)), label='Dense 32', bins=100, color=colors[0])\n",
    "plt.hist(final_encoder.layers[5].get_weights()[0].reshape((32*16,)), label='Dense 16',bins=100, color=colors[1])\n",
    "plt.hist(final_encoder.layers[8].get_weights()[0].reshape((16*3,)), label='Mu',bins=100, color=colors[2])\n",
    "plt.hist(final_encoder.layers[9].get_weights()[0].reshape((16*3,)), label='Sigma',bins=100, color=colors[3])\n",
    "#plt.yscale('log', nonpositive='clip')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Weights')\n",
    "plt.ylabel('Number of Weights')\n",
    "plt.title('Not pruned')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training/validation loss\n",
    "MSE & KL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = np.array(history.history['loss'][:])\n",
    "kl_loss_train = np.array(history.history['kl_loss'][:])\n",
    "loss_val = np.array(history.history['val_loss'][:])\n",
    "kl_loss_val = np.array(history.history['val_kl_loss'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_train-kl_loss_train, label='Training loss')\n",
    "plt.plot(loss_val-kl_loss_val, label='Validation loss')\n",
    "plt.title('Training and validation loss - MSE')\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['kl_loss'][:], label='Training loss')\n",
    "plt.plot(history.history['val_kl_loss'][:], label='Validation loss')\n",
    "plt.title('Training and validation KL loss')\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction - background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Delphes_QCD_BSM_data_half2.pkl', 'rb') as f:\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd_mean, qcd_logvar, qcd_z = final_encoder.predict(X_test_flatten)\n",
    "qcd_prediction = final_decoder.predict(qcd_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction - Beyond Standard Model events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_labels = ['Leptoquark','A to 4 leptons', 'hChToTauNu', 'hToTauTau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_results = []\n",
    "\n",
    "for i, label in enumerate(bsm_labels[:]):\n",
    "    mean_pred, logvar_pred, z_pred = final_encoder.predict(bsm_data[i])\n",
    "    bsm_prediction = final_decoder.predict(z_pred)\n",
    "    bsm_results.append([label, bsm_target[i], bsm_prediction, mean_pred, logvar_pred, z_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = 'VAE_result_pruned.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(output_result, 'w')\n",
    "h5f.create_dataset('QCD', data = X_test_scaled)\n",
    "h5f.create_dataset('QCD_input', data=X_test_flatten)\n",
    "h5f.create_dataset('predicted_QCD', data = qcd_prediction)\n",
    "h5f.create_dataset('encoded_mean_QCD', data = qcd_mean)\n",
    "h5f.create_dataset('encoded_logvar_QCD', data = qcd_logvar)\n",
    "h5f.create_dataset('encoded_z_QCD', data = qcd_z)\n",
    "for i, bsm in enumerate(bsm_results):\n",
    "    h5f.create_dataset('%s_scaled' %bsm[0], data=bsm[1])\n",
    "    h5f.create_dataset('%s_input' %bsm[0], data=bsm_data[i])\n",
    "    h5f.create_dataset('predicted_%s' %bsm[0], data=bsm[2])\n",
    "    h5f.create_dataset('encoded_mean_%s' %bsm[0], data=bsm[3])\n",
    "    h5f.create_dataset('encoded_logvar_%s' %bsm[0], data=bsm[4])\n",
    "    h5f.create_dataset('encoded_z_%s' %bsm[0], data=bsm[5])\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "neptune": {
   "notebookId": "db31bc2a-7b92-40f5-8253-dfc65381a6fb",
   "projectVersion": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
