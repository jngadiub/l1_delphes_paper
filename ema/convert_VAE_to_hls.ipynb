{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import setGPU\n",
    "import hls4ml\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, Reshape\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from qkeras import QDense, QActivation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from custom_layers import KLLoss\n",
    "from functions import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/eos/user/e/epuljak/autoencoder_models/test_data.pkl', 'rb') as f:\n",
    "    X_test_flatten, bsm_data, _ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_size=8\n",
    "custom_objects={'QDense': QDense, 'QActivation': QActivation, 'KLLoss': KLLoss}\n",
    "encoder = load_model('/eos/user/e/epuljak/autoencoder_models/VAE_encoder_PTQ_qkeras8', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware = 'xcvu9p-flgb2104-2-e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = hls4ml.utils.config_from_keras_model(encoder, default_precision='ap_fixed<16,6,AP_RND_CONV,AP_SAT>',\n",
    "        granularity='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update config\n",
    "config['LayerName']['input_1'].update({\n",
    "        'Precision': 'ap_fixed<22,12,AP_RND_CONV,AP_SAT>'\n",
    "        })\n",
    "config['LayerName']['q_activation']['Precision']['result'] = 'ap_fixed<16,11,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['batch_normalization']['Precision']['scale'] = 'ap_fixed<16,8>'\n",
    "config['LayerName']['batch_normalization']['Precision']['bias'] = 'ap_fixed<16,4>'\n",
    "config['LayerName']['batch_normalization']['Precision']['result'] = 'ap_fixed<22,10,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense']['Precision']['result'] = 'ap_fixed<16,10,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense']['Precision']['accum'] = 'ap_fixed<16,10,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['batch_normalization_1']['Precision']['scale'] = 'ap_fixed<8,2,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['batch_normalization_1']['Precision']['bias'] = 'ap_fixed<8,1,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['batch_normalization_1']['Precision']['result'] = 'ap_fixed<16,10,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_activation_1']['Precision']['result'] = 'ap_fixed<9,4,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense_1']['Precision']['result'] = 'ap_fixed<16,7,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense_1']['Precision']['accum'] = 'ap_fixed<16,7,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['batch_normalization_2']['Precision']['scale'] = 'ap_fixed<8,2,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['batch_normalization_2']['Precision']['bias'] = 'ap_fixed<8,1,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_activation_2']['Precision']['result'] = 'ap_fixed<9,4,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense_2']['Precision']['weight'] = 'ap_fixed<8,-1,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense_2']['Precision']['bias'] = 'ap_fixed<8,-1,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense_2']['Precision']['result'] = 'ap_fixed<18,3,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense_2']['Precision']['accum'] = 'ap_fixed<18,3,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense_3']['Precision']['weight'] = 'ap_fixed<8,-1,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense_3']['Precision']['bias'] = 'ap_fixed<8,-1,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense_3']['Precision']['result'] = 'ap_fixed<18,3,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['q_dense_3']['Precision']['accum'] = 'ap_fixed<18,3,AP_RND_CONV,AP_SAT>'\n",
    "config['LayerName']['kl_loss'].update({\n",
    "        'Precision': {\n",
    "            'accum': 'ap_fixed<32,10,AP_RND,AP_SAT>',\n",
    "            'result': 'ap_fixed<32,10,AP_RND,AP_SAT>'\n",
    "        },\n",
    "        'sum_t': 'ap_fixed<32,10>',\n",
    "        'exp_range': 0.5,\n",
    "        'exp_table_t': 'ap_fixed<32,10,AP_RND,AP_SAT>',\n",
    "        'table_size': 1024*4\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(d, indent=0):\n",
    "    align=20\n",
    "    for key, value in d.items():\n",
    "        print('  ' * indent + str(key), end='')\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            print_dict(value, indent+1)\n",
    "        else:\n",
    "            print(':' + ' ' * (20 - len(key) - 2 * indent) + str(value))\n",
    "\n",
    "print_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in config['LayerName'].keys():\n",
    "    config['LayerName'][layer]['Trace'] = True\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(encoder,\n",
    "                                                       hls_config=config,\n",
    "                                                       output_dir='output/DVAE_PTQ/xcvu9p-2/',\n",
    "                                                       fpga_part=hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file='ptq_VAE_qkeras_%d.pdf'%quant_size)\n",
    "hls4ml.model.profiling.numerical(model=encoder, hls_model=hls_model, X=X_test_flatten[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ca19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.model.profiling.compare(keras_model=encoder, hls_model=hls_model, X=X_test_flatten[:100000], plot_type='norm_diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-michigan",
   "metadata": {},
   "source": [
    "## CHECK ROCs Keras vs HLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder.predict(X_test_flatten)\n",
    "y_hls = hls_model.predict(X_test_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for KL layer output\n",
    "kl_loss_total = []\n",
    "kl_loss_total.append(y) #keras\n",
    "kl_loss_total.append(y_hls) #hls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_labels = ['Leptoquark','A to 4 leptons', 'hChToTauNu', 'hToTauTau']\n",
    "labels = ['QCD keras', 'QCD hls',\\\n",
    "          r'QKeras LQ $\\rightarrow$ b$\\tau$', r'HLS LQ $\\rightarrow$ b$\\tau$',\\\n",
    "          r'QKeras A $\\rightarrow$ 4L', r'HLS A $\\rightarrow$ 4L',\\\n",
    "          r'QKeras $h_{\\pm} \\rightarrow \\tau\\nu$', r'HLS $h_{\\pm} \\rightarrow \\tau\\nu$',\\\n",
    "          r'QKeras $h_{0} \\rightarrow \\tau\\tau$', r'HLS $h_{0} \\rightarrow \\tau\\tau$']\n",
    "loss = '$D_{KL}$'\n",
    "\n",
    "colors = ['C1','C2', 'C3', 'C4', 'C5', 'C6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(bsm_labels):\n",
    "    hls4ml_pred = hls_model.predict(bsm_data[i])\n",
    "    keras_pred = encoder.predict(bsm_data[i])\n",
    "    \n",
    "    kl_loss_total.append(keras_pred) #keras\n",
    "    kl_loss_total.append(hls4ml_pred) #hls\n",
    "    print(\"========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "minScore = 999999.\n",
    "maxScore = 0\n",
    "for i in range(len(labels)):\n",
    "    thisMin = np.min(kl_loss_total[i])\n",
    "    thisMax = np.max(kl_loss_total[i])\n",
    "    minScore = min(thisMin, minScore)\n",
    "    maxScore = max(maxScore, thisMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size=100\n",
    "plt.figure(figsize=(10,8))\n",
    "z = 0\n",
    "for i, label in enumerate(labels):\n",
    "    if i%2==0:\n",
    "        plt.hist(kl_loss_total[i].reshape(kl_loss_total[i].shape[0]*1), bins=bin_size, label=label, density = True, range=(minScore, maxScore),\n",
    "         histtype='step', fill=False, linewidth=1.5, color=colors[z])\n",
    "    if i%2==1:\n",
    "        plt.hist(kl_loss_total[i].reshape(kl_loss_total[i].shape[0]*1), bins=bin_size, label=label, density = True, range=(minScore, maxScore),\n",
    "         histtype='step', fill=False, linewidth=1.5, alpha=0.6, color=colors[z])\n",
    "        z = z+1\n",
    "#plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Probability (a.u.)\")\n",
    "plt.grid(True)\n",
    "plt.title('KL loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "tpr_lq=[];fpr_lq=[];auc_lq=[]\n",
    "tpr_ato4l=[];fpr_ato4l=[];auc_ato4l=[]\n",
    "tpr_ch=[];fpr_ch=[];auc_ch=[]\n",
    "tpr_to=[];fpr_to=[];auc_to=[]\n",
    "\n",
    "\n",
    "target_qcd = np.zeros(kl_loss_total[0].shape[0])\n",
    "target_qcd_hls = np.zeros(kl_loss_total[1].shape[0])\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if i == 0 and i==1: continue\n",
    "    if i%2==0:\n",
    "        trueVal = np.concatenate((np.ones(kl_loss_total[i].shape[0]), target_qcd))\n",
    "        predVal_loss = np.concatenate((kl_loss_total[i], kl_loss_total[0]))\n",
    "\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "        if i==2:\n",
    "            tpr_lq.append(tpr_loss)\n",
    "            fpr_lq.append(fpr_loss)\n",
    "            auc_lq.append(auc_loss)\n",
    "        elif i == 4:\n",
    "            tpr_ato4l.append(tpr_loss)\n",
    "            fpr_ato4l.append(fpr_loss)\n",
    "            auc_ato4l.append(auc_loss)\n",
    "        elif i==6:\n",
    "            tpr_ch.append(tpr_loss)\n",
    "            fpr_ch.append(fpr_loss)\n",
    "            auc_ch.append(auc_loss)\n",
    "        elif i == 8:\n",
    "            tpr_to.append(tpr_loss)\n",
    "            fpr_to.append(fpr_loss)\n",
    "            auc_to.append(auc_loss)\n",
    "    if i%2==1:\n",
    "        \n",
    "        trueVal = np.concatenate((np.ones(kl_loss_total[i].shape[0]), target_qcd_hls))\n",
    "        predVal_loss = np.concatenate((kl_loss_total[i], kl_loss_total[1]))\n",
    "\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "        if i==3:\n",
    "            tpr_lq.append(tpr_loss)\n",
    "            fpr_lq.append(fpr_loss)\n",
    "            auc_lq.append(auc_loss)\n",
    "        elif i == 5:\n",
    "            tpr_ato4l.append(tpr_loss)\n",
    "            fpr_ato4l.append(fpr_loss)\n",
    "            auc_ato4l.append(auc_loss)\n",
    "        elif i==7:\n",
    "            tpr_ch.append(tpr_loss)\n",
    "            fpr_ch.append(fpr_loss)\n",
    "            auc_ch.append(auc_loss)\n",
    "        elif i == 9:\n",
    "            tpr_to.append(tpr_loss)\n",
    "            fpr_to.append(fpr_loss)\n",
    "            auc_to.append(auc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_lq[:], fpr_lq[:], auc_lq[:], labels[2:4])):\n",
    "    if i == 1:\n",
    "        plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[0], alpha=0.6, linestyle='dashed')\n",
    "    else: \n",
    "        plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[0])\n",
    "\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_ato4l[:], fpr_ato4l[:], auc_ato4l[:], labels[4:6])):\n",
    "    if i == 1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[1], alpha = 0.6, linestyle='dashed')\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[1])\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_ch[:], fpr_ch[:], auc_ch[:], labels[6:8])):\n",
    "    if i==1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[2], alpha=0.6, linestyle='dashed')\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[2])\n",
    "\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_to[:], fpr_to[:], auc_to[:], labels[8:])):\n",
    "    if i==1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[3], alpha=0.6, linestyle='dashed')\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[3])\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=[1.2, 0.5],loc='best',frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
