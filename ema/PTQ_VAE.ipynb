{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import setGPU\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from qkeras import QDense, QActivation\n",
    "from qkeras import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from functions import preprocess_anomaly_data, custom_loss_negative, custom_loss_training,\\\n",
    "roc_objective,load_model, save_model\n",
    "from custom_layers import Sampling\n",
    "\n",
    "from autoencoder_classes import VAE\n",
    "\n",
    "tsk = tfmot.sparsity.keras\n",
    "# from tensorflow.python.client import device_lib \n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = (N,19,3,1).flatten()\n",
    "with open('/eos/user/e/epuljak/forDelphes/Delphes_QCD_BSM_data.pkl', 'rb') as f:\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_size = 8\n",
    "integer = 3\n",
    "symmetric = 1\n",
    "alpha=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-california",
   "metadata": {},
   "source": [
    "### Define model\n",
    "Prune and quantize only encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "input_shape = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "#proba\n",
    "x = QActivation(f'quantized_bits(16,10,1,alpha=1)')(inputArray)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x) if quant_size==0\\\n",
    "    else QDense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "               kernel_quantizer=f'quantized_bits(bits=' + str(quant_size) + ', integer=' + str(integer) + ', symmetric=1, alpha=1)',\\\n",
    "               bias_quantizer=f'quantized_bits(bits=' + str(quant_size) + ', integer=' + str(integer) + ', symmetric=1, alpha=1)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x) if quant_size==0\\\n",
    "    else QActivation(f'quantized_relu(bits=' + str(quant_size) + ', integer=' + str(integer) + ')')(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x) if quant_size==0\\\n",
    "    else QDense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "               kernel_quantizer=f'quantized_bits(bits=' + str(quant_size) + ', integer=' + str(integer) + ', symmetric=1, alpha=1)',\\\n",
    "               bias_quantizer=f'quantized_bits(bits=' + str(quant_size) + ', integer=' + str(integer) + ', symmetric=1, alpha=1)')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x) if quant_size==0\\\n",
    "    else QActivation(f'quantized_relu(bits=' + str(quant_size) + ', integer=' + str(integer) + ')')(x)\n",
    "mu = Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x) if quant_size==0\\\n",
    "    else QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "               kernel_quantizer=f'quantized_bits(bits=' + str(16) + ', integer=' + str(6) + ', symmetric=1, alpha=1)',\\\n",
    "               bias_quantizer=f'quantized_bits(bits=' + str(16) + ', integer=' + str(6) + ', symmetric=1, alpha=1)')(x)\n",
    "logvar = Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x) if quant_size==0\\\n",
    "    else QDense(latent_dim, kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\\\n",
    "               kernel_quantizer=f'quantized_bits(bits=' + str(16) + ', integer=' + str(6) + ', symmetric=1, alpha=1)',\\\n",
    "               bias_quantizer=f'quantized_bits(bits=' + str(16) + ', integer=' + str(6) + ', symmetric=1, alpha=1)')(x)\n",
    "# Use reparameterization trick to ensure correct gradient\n",
    "z = Sampling()([mu, logvar])\n",
    "\n",
    "# Create encoder\n",
    "encoder = Model(inputArray, [mu, logvar, z], name='encoder')    \n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "#decoder\n",
    "d_input = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(d_input)\n",
    "x = BatchNormalization()(x)\n",
    "#x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x)    \n",
    "x = BatchNormalization()(x)\n",
    "#x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Activation('relu')(x)\n",
    "dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(x)\n",
    "# Create decoder\n",
    "decoder = Model(d_input, dec, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BP or higher bit width model\n",
    "model_dir = 'VAE_models/final_models/withCorrectPrefiltering/'\n",
    "name_encoder ='VAE_encoder_pruned'\n",
    "name_decoder ='VAE_decoder_pruned'\n",
    "custom_objects={'Sampling': Sampling}\n",
    "\n",
    "BP_encoder = load_model(model_dir+name_encoder, custom_objects)\n",
    "BP_decoder = load_model(model_dir+name_decoder, custom_objects)\n",
    "\n",
    "#BP_encoder, BP_decoder = VAE.load(model_dir, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set weights for encoder\n",
    "for i, l in enumerate(vae.encoder.layers):\n",
    "    if i < 2: continue\n",
    "    vae.encoder.layers[i].set_weights(BP_encoder.layers[i-1].get_weights()) # i-1 because of QActivation layer (remove when loading from qkeras model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set weights for encoder\n",
    "#vae.decoder.load_weights('/eos/user/e/epuljak/autoencoder_models/VAE_decoder_pruned.h5')\n",
    "for i, l in enumerate(vae.decoder.layers):\n",
    "    if i == 0: continue\n",
    "    vae.decoder.layers[i].set_weights(BP_decoder.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check quantizers\n",
    "for layer in vae.encoder.layers:\n",
    "    if hasattr(layer, \"kernel_quantizer\"):\n",
    "        print(layer.name, \"kernel:\", str(layer.kernel_quantizer_internal),\" scale:\",str(layer.kernel_quantizer_internal.scale), \"bias:\", str(layer.bias_quantizer_internal))\n",
    "    elif hasattr(layer, \"quantizer\"):\n",
    "        print(layer.name, \"quantizer:\", str(layer.quantizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-detection",
   "metadata": {},
   "source": [
    "## Check range of mean and logvar in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, logvar, _ = vae.encoder.predict(X_test_flatten[:10000])\n",
    "\n",
    "plt.hist(logvar[:,0], bins=100, range=(-0.2,0.2))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(logvar[:,1], bins=100, range=(-0.2,0.2))\n",
    "plt.show()\n",
    "plt.hist(logvar[:,2], bins=100, range=(-0.2,0.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-thing",
   "metadata": {},
   "source": [
    "## Add custom KL layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers import KLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only encoder\n",
    "#model = final_encoder\n",
    "# get mu and sigma from model\n",
    "z_mean = vae.encoder.layers[-3].output\n",
    "z_log_var = vae.encoder.layers[-2].output\n",
    "# calculate KL distance with the custom layer\n",
    "custom_output = KLLoss()([z_mean, z_log_var])\n",
    "# create new model\n",
    "model = Model(inputs=vae.encoder.input, outputs=custom_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model('VAE_models/final_models/withCorrectPrefiltering/VAE_encoder_PTQ', model)\n",
    "\n",
    "\n",
    "model = load_model('VAE_models/final_models/withCorrectPrefiltering/VAE_encoder_PTQ', custom_objects={'KLLoss': KLLoss, 'QActivation': QActivation, 'QDense': QDense})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-irish",
   "metadata": {},
   "source": [
    "## Convert to HLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_diff_layer_by_layer(layer_keras, layer_hls):\n",
    "    print(f'Keras layer {layer_hls}, first sample:')\n",
    "    print(config['LayerName'][layer_hls])\n",
    "    print(keras_trace[layer_keras][:].flatten()[:])\n",
    "    print(hls4ml_trace[layer_hls][:].flatten()[:])\n",
    "    print(keras_trace[layer_keras][:].flatten()[:]-hls4ml_trace[layer_hls][:].flatten()[:])\n",
    "    plt.hist(keras_trace[layer_keras][:].flatten()[:]-hls4ml_trace[layer_hls][:].flatten()[:], bins=100, range=(-1,1))\n",
    "    plt.title(layer_hls)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['Activation', 'KLLoss']\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND_CONV'\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware = 'xcvu9p-flgb2104-2-e'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-press",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = hls4ml.utils.config_from_keras_model(model, default_precision='ap_fixed<16,6,AP_RND_CONV,AP_SAT>',\n",
    "        max_bits=20,\n",
    "        data_type_mode='auto_accum', # auto_accum_only\n",
    "        granularity='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to skip merging dense+batchnorm layer in hls\n",
    "config['SkipOptimizers'] = ['fuse_batch_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Model']['Strategy'] = 'Resource'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update config\n",
    "config['LayerName']['kl_loss'].update({\n",
    "        'Precision': {\n",
    "            'accum': 'ap_fixed<32,10,AP_RND,AP_SAT>',\n",
    "            'result': 'ap_fixed<32,10,AP_RND,AP_SAT>'\n",
    "        },\n",
    "        'sum_t': 'ap_fixed<32,10>',\n",
    "        'exp_range': 0.5,\n",
    "        'exp_table_t': 'ap_fixed<32,10,AP_RND,AP_SAT>',\n",
    "        'table_size': 1024*4\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['LayerName']['input_1'].update({\n",
    "        'Precision': 'ap_fixed<16,10>'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['LayerName']['latent_mu']['Precision'].update({\n",
    "                'result': 'ap_fixed<32,6>'\n",
    "            })\n",
    "config['LayerName']['latent_logvar']['Precision'].update({\n",
    "        'result': 'ap_fixed<32,6>'\n",
    "    })\n",
    "config['LayerName']['latent_mu_linear'] = {\n",
    "        'Precision': 'ap_fixed<32, 6, AP_RND_CONV, AP_SAT>'\n",
    "    }\n",
    "config['LayerName']['latent_logvar_linear'] = {\n",
    "        'Precision': 'ap_fixed<32, 6, AP_RND_CONV, AP_SAT>'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['LayerName']['batch_normalization'].update({\n",
    "               'Precision': {'scale': 'ap_fixed<16,7>',\n",
    "                             'bias': 'ap_fixed<18,3>',\n",
    "                            'result': 'ap_fixed<16,6>'}\n",
    "                \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['LayerName']['q_dense'].update({\n",
    "               'Precision': {'result': 'ap_fixed<14,4>',\n",
    "                                'weight': 'ap_fixed<12,5>',\n",
    "                                'bias': 'ap_fixed<12,5>',\n",
    "                                'accum': 'ap_fixed<32,16,AP_RND,AP_SAT>'}\n",
    "                \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['LayerName']['q_dense_1'].update({\n",
    "               'Precision': {'result': 'ap_fixed<16,6>',\n",
    "                                'weight': 'ap_fixed<12,5>',\n",
    "                                'bias': 'ap_fixed<12,5>',\n",
    "                                'accum': 'ap_fixed<32,16,AP_RND,AP_SAT>'}\n",
    "                \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['LayerName']['batch_normalization_1'].update({\n",
    "               'Precision': {'scale': 'ap_fixed<3,2>',\n",
    "                                'bias': 'ap_fixed<8,1>',\n",
    "                                'result': 'ap_fixed<16,6,AP_RND_CONV,AP_SAT>'}\n",
    "                \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set precision for mean layer\n",
    "config['LayerName']['q_dense_2'].update({\n",
    "               'Precision': {'result': 'ap_fixed<16,6>',\n",
    "                            'weight': 'ap_fixed<16,7>',\n",
    "                            'bias': 'ap_fixed<16,7>',\n",
    "                            'accum': 'ap_fixed<32,8,AP_RND,AP_SAT>'}\n",
    "                \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set precision for logvar layer\n",
    "\n",
    "config['LayerName']['q_dense_3'].update({\n",
    "               'Precision': {'result': 'ap_fixed<16,6>',\n",
    "                            'weight': 'ap_fixed<16,7>',\n",
    "                            'bias': 'ap_fixed<16,7>',\n",
    "                            'accum': 'ap_fixed<32,8,AP_RND,AP_SAT>'}\n",
    "                \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hls4ml.utils.config import set_accum_from_keras_model\n",
    "set_accum_from_keras_model(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save config\n",
    "output_config = 'VAE_models/final_models/withCorrectPrefiltering/VAE_config_HLS.pkl'\n",
    "# with open(output_config, 'wb') as handle:\n",
    "#     pickle.dump(config, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(output_config, 'rb') as handle:\n",
    "    config = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model = hls4ml.converters.convert_from_keras_model(model,\n",
    "                                                       hls_config=config,\n",
    "                                                       output_dir='output/DVAE_PTQ/xcvu9p/',\n",
    "                                                       fpga_part=hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.model.profiling.numerical(model=model, hls_model=hls_model, X=X_test_flatten[:100000])\n",
    "hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file='ptq_VAE_qkeras_%d.pdf'%quant_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-canvas",
   "metadata": {},
   "source": [
    "## TRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in config['LayerName'].keys():\n",
    "    config['LayerName'][layer]['Trace'] = True\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(model,\n",
    "                                                       hls_config=config,\n",
    "                                                       output_dir='output/DVAE_PTQ/xcvu9p/',\n",
    "                                                       fpga_part=hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile() #after you compile hls model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-maria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hls4ml_pred, hls4ml_trace = hls_model.trace(X_test_flatten[:100000])\n",
    "keras_trace = hls4ml.model.profiling.get_ymodel_keras(model, X_test_flatten[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "hls4ml_trace.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "keras_trace.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in hls4ml_trace.keys():\n",
    "    if layer in keras_trace.keys():\n",
    "        print(f'Keras layer {layer}, first sample:')\n",
    "        print(config['LayerName'][layer])\n",
    "        print(keras_trace[layer][:].flatten()[:])\n",
    "        print(hls4ml_trace[layer][:].flatten()[:])\n",
    "        print(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:])\n",
    "        plt.hist(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:], bins=100, range=(-1,1))\n",
    "        plt.title(layer)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_diff_layer_by_layer('latent_mu_function', 'latent_mu_linear')\n",
    "check_diff_layer_by_layer('latent_logvar_function', 'latent_logvar_linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in hls4ml_trace.keys():\n",
    "    plt.figure()\n",
    "    klayer = layer\n",
    "    if '_alpha' in layer:\n",
    "        klayer = layer.replace('_alpha','')\n",
    "    plt.scatter(hls4ml_trace[layer].flatten(), keras_trace[klayer].flatten(), s=0.2)\n",
    "    min_x = min(np.amin(hls4ml_trace[layer]), np.amin(keras_trace[klayer]))\n",
    "    max_x = max(np.amax(hls4ml_trace[layer]), np.amax(keras_trace[klayer]))\n",
    "    plt.plot([min_x, max_x], [min_x, max_x], c='gray')\n",
    "    plt.xlabel('hls4ml {}'.format(layer))\n",
    "    plt.ylabel('QKeras {}'.format(klayer))\n",
    "    plt.show()\n",
    "    #plt.savefig('profiling_{}.png'.format(layer), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'batch_normalization'\n",
    "print(f'Keras layer batch_normalization, first sample:')\n",
    "#print(config['LayerName'][layer])\n",
    "print(keras_trace[layer][:].flatten()[:])\n",
    "print(hls4ml_trace[layer][:].flatten()[:])\n",
    "print(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:])\n",
    "plt.hist(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:], bins=100, range=(-1,1))\n",
    "plt.title(layer)\n",
    "plt.show()\n",
    "\n",
    "layer = 'dense'\n",
    "print(f'Keras layer dense, first sample:')\n",
    "#print(config['LayerName'][layer])\n",
    "print(keras_trace['batch_normalization_1'][:].flatten()[:])\n",
    "print(hls4ml_trace[layer][:].flatten()[:])\n",
    "print(keras_trace['batch_normalization_1'][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:])\n",
    "plt.hist(keras_trace['batch_normalization_1'][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:], bins=100, range=(-1,1))\n",
    "plt.title(layer)\n",
    "plt.show()\n",
    "\n",
    "layer = 'leaky_re_lu'\n",
    "print(f'Keras layer leaky_re_lu, first sample:')\n",
    "#print(config['LayerName'][layer])\n",
    "print(keras_trace[layer][:].flatten()[:])\n",
    "print(hls4ml_trace[layer][:].flatten()[:])\n",
    "print(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:])\n",
    "plt.hist(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:], bins=100, range=(-1,1))\n",
    "plt.title(layer)\n",
    "plt.show()\n",
    "\n",
    "layer = 'dense_1'\n",
    "print(f'Keras layer dense, first sample:')\n",
    "#print(config['LayerName'][layer])\n",
    "print(keras_trace['batch_normalization_2'][:].flatten()[:])\n",
    "print(hls4ml_trace[layer][:].flatten()[:])\n",
    "print(keras_trace['batch_normalization_2'][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:])\n",
    "plt.hist(keras_trace['batch_normalization_2'][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:], bins=100, range=(-1,1))\n",
    "plt.title(layer)\n",
    "plt.show()\n",
    "\n",
    "layer = 'leaky_re_lu_1'\n",
    "print(f'Keras layer leaky_re_lu, first sample:')\n",
    "#print(config['LayerName'][layer])\n",
    "print(keras_trace[layer][:].flatten()[:])\n",
    "print(hls4ml_trace[layer][:].flatten()[:])\n",
    "print(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:])\n",
    "plt.hist(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:], bins=100, range=(-1,1))\n",
    "plt.title(layer)\n",
    "plt.show()\n",
    "\n",
    "layer = 'latent_mu'\n",
    "print(f'Keras layer leaky_re_lu, first sample:')\n",
    "#print(config['LayerName'][layer])\n",
    "print(keras_trace[layer][:].flatten()[:])\n",
    "print(hls4ml_trace[layer][:].flatten()[:])\n",
    "print(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:])\n",
    "plt.hist(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:], bins=100, range=(-1,1))\n",
    "plt.title(layer)\n",
    "plt.show()\n",
    "\n",
    "layer = 'latent_logvar'\n",
    "print(f'Keras layer leaky_re_lu, first sample:')\n",
    "#print(config['LayerName'][layer])\n",
    "print(keras_trace[layer][:].flatten()[:])\n",
    "print(hls4ml_trace[layer][:].flatten()[:])\n",
    "print(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:])\n",
    "plt.hist(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:], bins=100, range=(-1,1))\n",
    "plt.title(layer)\n",
    "plt.show()\n",
    "\n",
    "layer = 'kl_loss'\n",
    "print(f'Keras layer leaky_re_lu, first sample:')\n",
    "#print(config['LayerName'][layer])\n",
    "print(keras_trace[layer][:].flatten()[:])\n",
    "print(hls4ml_trace[layer][:].flatten()[:])\n",
    "print(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:])\n",
    "plt.hist(keras_trace[layer][:].flatten()[:]-hls4ml_trace[layer][:].flatten()[:], bins=100, range=(-1,1))\n",
    "plt.title(layer)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove linear nodes from the network\n",
    "for l in list(hls_model.get_layers()):\n",
    "    if '_linear' in l.name: hls_model.remove_node(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.model.profiling.compare(model, hls_model, X_test_flatten[:100000], 'dist_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.model.profiling.compare(model, hls_model, X_test_flatten[:1000000], 'norm_diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-ideal",
   "metadata": {},
   "source": [
    "## CHECK ROCs Keras vs HLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras model predictions\n",
    "y = model.predict(X_test_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HLS model predictions\n",
    "y_hls = hls_model.predict(X_test_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for KL layer output\n",
    "kl_loss_total = []\n",
    "kl_loss_total.append(y)\n",
    "kl_loss_total.append(y_hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_labels = ['Leptoquark','A to 4 leptons', 'hChToTauNu', 'hToTauTau']\n",
    "labels = ['QCD keras', 'QCD hls',\\\n",
    "          r'QKeras LQ $\\rightarrow$ b$\\tau$', r'HLS LQ $\\rightarrow$ b$\\tau$',\\\n",
    "          r'QKeras A $\\rightarrow$ 4L', r'HLS A $\\rightarrow$ 4L',\\\n",
    "          r'QKeras $h_{\\pm} \\rightarrow \\tau\\nu$', r'HLS $h_{\\pm} \\rightarrow \\tau\\nu$',\\\n",
    "          r'QKeras $h_{0} \\rightarrow \\tau\\tau$', r'HLS $h_{0} \\rightarrow \\tau\\tau$']\n",
    "loss = '$D_{KL}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(bsm_labels):\n",
    "    hls4ml_pred = hls_model.predict(bsm_data[i])\n",
    "    keras_pred = model.predict(bsm_data[i])\n",
    "    \n",
    "    kl_loss_total.append(keras_pred)\n",
    "    kl_loss_total.append(hls4ml_pred)\n",
    "    print(\"========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "minScore = 999999.\n",
    "maxScore = 0\n",
    "for i in range(len(labels)):\n",
    "    thisMin = np.min(kl_loss_total[i])\n",
    "    thisMax = np.max(kl_loss_total[i])\n",
    "    minScore = min(thisMin, minScore)\n",
    "    maxScore = max(maxScore, thisMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['C1','C2', 'C3', 'C4', 'C5', 'C6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size=100\n",
    "plt.figure(figsize=(10,8))\n",
    "z = 0\n",
    "for i, label in enumerate(labels):\n",
    "    if i%2==0:\n",
    "        plt.hist(kl_loss_total[i].reshape(kl_loss_total[i].shape[0]*1), bins=bin_size, label=label, density = True, range=(minScore, maxScore),\n",
    "         histtype='step', fill=False, linewidth=1.5, color=colors[z])\n",
    "    if i%2==1:\n",
    "        plt.hist(kl_loss_total[i].reshape(kl_loss_total[i].shape[0]*1), bins=bin_size, label=label, density = True, range=(minScore, maxScore),\n",
    "         histtype='step', fill=False, linewidth=1.5, alpha=0.6, color=colors[z])\n",
    "        z = z+1\n",
    "#plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Probability (a.u.)\")\n",
    "plt.grid(True)\n",
    "plt.title('KL loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "tpr_lq=[];fpr_lq=[];auc_lq=[]\n",
    "tpr_ato4l=[];fpr_ato4l=[];auc_ato4l=[]\n",
    "tpr_ch=[];fpr_ch=[];auc_ch=[]\n",
    "tpr_to=[];fpr_to=[];auc_to=[]\n",
    "\n",
    "\n",
    "target_qcd = np.zeros(kl_loss_total[0].shape[0])\n",
    "target_qcd_hls = np.zeros(kl_loss_total[1].shape[0])\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if i == 0 and i==1: continue\n",
    "    if i%2==0:\n",
    "        trueVal = np.concatenate((np.ones(kl_loss_total[i].shape[0]), target_qcd))\n",
    "        predVal_loss = np.concatenate((kl_loss_total[i], kl_loss_total[0]))\n",
    "\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "        if i==2:\n",
    "            tpr_lq.append(tpr_loss)\n",
    "            fpr_lq.append(fpr_loss)\n",
    "            auc_lq.append(auc_loss)\n",
    "        elif i == 4:\n",
    "            tpr_ato4l.append(tpr_loss)\n",
    "            fpr_ato4l.append(fpr_loss)\n",
    "            auc_ato4l.append(auc_loss)\n",
    "        elif i==6:\n",
    "            tpr_ch.append(tpr_loss)\n",
    "            fpr_ch.append(fpr_loss)\n",
    "            auc_ch.append(auc_loss)\n",
    "        elif i == 8:\n",
    "            tpr_to.append(tpr_loss)\n",
    "            fpr_to.append(fpr_loss)\n",
    "            auc_to.append(auc_loss)\n",
    "    if i%2==1:\n",
    "        \n",
    "        trueVal = np.concatenate((np.ones(kl_loss_total[i].shape[0]), target_qcd_hls))\n",
    "        predVal_loss = np.concatenate((kl_loss_total[i], kl_loss_total[1]))\n",
    "\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "        if i==3:\n",
    "            tpr_lq.append(tpr_loss)\n",
    "            fpr_lq.append(fpr_loss)\n",
    "            auc_lq.append(auc_loss)\n",
    "        elif i == 5:\n",
    "            tpr_ato4l.append(tpr_loss)\n",
    "            fpr_ato4l.append(fpr_loss)\n",
    "            auc_ato4l.append(auc_loss)\n",
    "        elif i==7:\n",
    "            tpr_ch.append(tpr_loss)\n",
    "            fpr_ch.append(fpr_loss)\n",
    "            auc_ch.append(auc_loss)\n",
    "        elif i == 9:\n",
    "            tpr_to.append(tpr_loss)\n",
    "            fpr_to.append(fpr_loss)\n",
    "            auc_to.append(auc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_lq[:], fpr_lq[:], auc_lq[:], labels[2:4])):\n",
    "    if i == 1:\n",
    "        plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[0], alpha=0.6, linestyle='dashed')\n",
    "    else: \n",
    "        plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[0])\n",
    "\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_ato4l[:], fpr_ato4l[:], auc_ato4l[:], labels[4:6])):\n",
    "    if i == 1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[1], alpha = 0.6, linestyle='dashed')\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[1])\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_ch[:], fpr_ch[:], auc_ch[:], labels[6:8])):\n",
    "    if i==1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[2], alpha=0.6, linestyle='dashed')\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[2])\n",
    "\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_to[:], fpr_to[:], auc_to[:], labels[8:])):\n",
    "    if i==1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[3], alpha=0.6, linestyle='dashed')\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[3])\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=[1.2, 0.5],loc='best',frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-export",
   "metadata": {},
   "source": [
    "# Check ROCs QKeras vs BP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/eos/user/e/epuljak/forDelphes/Delphes_QCD_BSM_data_half2.pkl', 'rb') as f:\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create BP model with KL layer\n",
    "\n",
    "# get mu and sigma from model\n",
    "z_mean = BP_encoder.layers[-3].output\n",
    "z_log_var = BP_encoder.layers[-2].output\n",
    "# calculate KL distance with the custom layer\n",
    "custom_output = KLLoss()([z_mean, z_log_var])\n",
    "# create new model\n",
    "BP_model = Model(inputs=BP_encoder.input, outputs=custom_output)\n",
    "BP_model.summary()\n",
    "BP_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_keras = model.predict(X_test_flatten)\n",
    "y_BP = BP_model.predict(X_test_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss_total = []\n",
    "kl_loss_total.append(y_keras)\n",
    "kl_loss_total.append(y_BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_labels = ['Leptoquark','A to 4 leptons', 'hChToTauNu', 'hToTauTau']\n",
    "labels = ['QCD QKeras', 'QCD BP Keras',\\\n",
    "          r'QKeras LQ $\\rightarrow$ b$\\tau$', r'BP Keras LQ $\\rightarrow$ b$\\tau$',\\\n",
    "          r'QKeras A $\\rightarrow$ 4L', r'BP Keras A $\\rightarrow$ 4L',\\\n",
    "          r'QKeras $h_{\\pm} \\rightarrow \\tau\\nu$', r'BP Keras $h_{\\pm} \\rightarrow \\tau\\nu$',\\\n",
    "          r'QKeras $h_{0} \\rightarrow \\tau\\tau$', r'BP Keras $h_{0} \\rightarrow \\tau\\tau$']\n",
    "loss = '$D_{KL}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(bsm_labels):\n",
    "    qkeras_pred = model.predict(bsm_data[i])\n",
    "    BP_pred = BP_model.predict(bsm_data[i])\n",
    "    \n",
    "    kl_loss_total.append(qkeras_pred)\n",
    "    kl_loss_total.append(BP_pred)\n",
    "    print(\"========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check range of loss distributions\n",
    "minScore = 999999.\n",
    "maxScore = 0\n",
    "for i in range(len(labels)):\n",
    "    thisMin = np.min(kl_loss_total[i])\n",
    "    thisMax = np.max(kl_loss_total[i])\n",
    "    minScore = min(thisMin, minScore)\n",
    "    maxScore = max(maxScore, thisMax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['C1','C2', 'C3', 'C4', 'C5', 'C6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size=100\n",
    "plt.figure(figsize=(10,8))\n",
    "z = 0\n",
    "for i, label in enumerate(labels):\n",
    "    if i%2==0:\n",
    "        plt.hist(kl_loss_total[i].reshape(kl_loss_total[i].shape[0]*1), bins=bin_size, label=label, density = True, range=(minScore, 2),\n",
    "         histtype='step', fill=False, linewidth=1.5, color=colors[z])\n",
    "    if i%2==1:\n",
    "        plt.hist(kl_loss_total[i].reshape(kl_loss_total[i].shape[0]*1), bins=bin_size, label=label, density = True, range=(minScore, 2),\n",
    "         histtype='step', fill=False, linewidth=1.5, alpha=0.6, color=colors[z])\n",
    "        z = z+1\n",
    "#plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.ylabel(\"Probability (a.u.)\")\n",
    "plt.grid(True)\n",
    "plt.title('KL loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "tpr_lq=[];fpr_lq=[];auc_lq=[]\n",
    "tpr_ato4l=[];fpr_ato4l=[];auc_ato4l=[]\n",
    "tpr_ch=[];fpr_ch=[];auc_ch=[]\n",
    "tpr_to=[];fpr_to=[];auc_to=[]\n",
    "\n",
    "\n",
    "target_qcd_qkeras = np.zeros(kl_loss_total[0].shape[0])\n",
    "target_qcd_BP = np.zeros(kl_loss_total[1].shape[0])\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if i == 0 and i==1: continue\n",
    "    if i%2==0:\n",
    "        trueVal = np.concatenate((np.ones(kl_loss_total[i].shape[0]), target_qcd_qkeras))\n",
    "        predVal_loss = np.concatenate((kl_loss_total[i], kl_loss_total[0]))\n",
    "\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "        if i==2:\n",
    "            tpr_lq.append(tpr_loss)\n",
    "            fpr_lq.append(fpr_loss)\n",
    "            auc_lq.append(auc_loss)\n",
    "        elif i == 4:\n",
    "            tpr_ato4l.append(tpr_loss)\n",
    "            fpr_ato4l.append(fpr_loss)\n",
    "            auc_ato4l.append(auc_loss)\n",
    "        elif i==6:\n",
    "            tpr_ch.append(tpr_loss)\n",
    "            fpr_ch.append(fpr_loss)\n",
    "            auc_ch.append(auc_loss)\n",
    "        elif i == 8:\n",
    "            tpr_to.append(tpr_loss)\n",
    "            fpr_to.append(fpr_loss)\n",
    "            auc_to.append(auc_loss)\n",
    "    if i%2==1:\n",
    "        \n",
    "        trueVal = np.concatenate((np.ones(kl_loss_total[i].shape[0]), target_qcd_BP))\n",
    "        predVal_loss = np.concatenate((kl_loss_total[i], kl_loss_total[1]))\n",
    "\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(trueVal, predVal_loss)\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "        if i==3:\n",
    "            tpr_lq.append(tpr_loss)\n",
    "            fpr_lq.append(fpr_loss)\n",
    "            auc_lq.append(auc_loss)\n",
    "        elif i == 5:\n",
    "            tpr_ato4l.append(tpr_loss)\n",
    "            fpr_ato4l.append(fpr_loss)\n",
    "            auc_ato4l.append(auc_loss)\n",
    "        elif i==7:\n",
    "            tpr_ch.append(tpr_loss)\n",
    "            fpr_ch.append(fpr_loss)\n",
    "            auc_ch.append(auc_loss)\n",
    "        elif i == 9:\n",
    "            tpr_to.append(tpr_loss)\n",
    "            fpr_to.append(fpr_loss)\n",
    "            auc_to.append(auc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_lq[:], fpr_lq[:], auc_lq[:], labels[2:4])):\n",
    "    if i == 1:\n",
    "        plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[0], alpha=0.6)\n",
    "    else: \n",
    "        plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[0])\n",
    "\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_ato4l[:], fpr_ato4l[:], auc_ato4l[:], labels[4:6])):\n",
    "    if i == 1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[1], alpha = 0.6)\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[1])\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_ch[:], fpr_ch[:], auc_ch[:], labels[6:8])):\n",
    "    if i==1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[2], alpha=0.6)\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[2])\n",
    "\n",
    "for i, (tpr, fpr, auc, L) in enumerate(zip(tpr_to[:], fpr_to[:], auc_to[:], labels[8:])):\n",
    "    if i==1: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[3], alpha=0.6)\n",
    "    else: plt.plot(fpr, tpr, \"-\", label='%s (auc = %.1f%%)'%(L,auc*100.), linewidth=1.5, color=colors[3])\n",
    "plt.semilogx()\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True)\n",
    "#plt.ylim(0,0.5)\n",
    "plt.legend(bbox_to_anchor=[1.2, 0.5],loc='best',frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='red', linestyle='dashed', linewidth=1)\n",
    "#plt.title('QKERAS <16,6>')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = '/eos/user/e/epuljak/forDelphes/CorrectDataResults/PTQ_VAE_result_qkeras2_1610.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(output_result, 'w')\n",
    "h5f.create_dataset('QCD_Qkeras', data = kl_loss_total[0])\n",
    "h5f.create_dataset('QCD_BP', data = kl_loss_total[1])\n",
    "for i,bsm in enumerate(bsm_labels[:]):\n",
    "    print(i)\n",
    "    if i == 0: z = 2\n",
    "    elif i == 1: z = 4\n",
    "    elif i == 2: z = 6\n",
    "    elif i == 3: z = 8\n",
    "    h5f.create_dataset('%s_Qkeras' %bsm, data = kl_loss_total[z])\n",
    "    h5f.create_dataset('%s_BP'%bsm, data = kl_loss_total[z+1])\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-increase",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
