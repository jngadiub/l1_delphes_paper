{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import setGPU\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from functions import preprocess_anomaly_data,\\\n",
    "roc_objective,load_model, save_model, make_mse_loss\n",
    "import pickle\n",
    "from autoencoder_classes import AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = (N,19,3,1).flatten()\n",
    "with open('/eos/user/e/epuljak/forDelphes/Delphes_QCD_BSM_data.pkl', 'rb') as f:\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "input_shape = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "x = BatchNormalization()(inputArray)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# encoder = LeakyReLU(alpha=0.3)(x)\n",
    "#decoder\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "#create autoencoder\n",
    "autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AE(autoencoder)\n",
    "ae.compile(optimizer=keras.optimizers.Adam(lr=0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "callbacks=[]\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(NeptuneMonitor())\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING\")\n",
    "history = ae.fit(X_train_flatten, X_train_scaled, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_split=0.2,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('AE_models/final_models/withCorrectPrefiltering/AE_notpruned', ae.autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.save('AE_Delphes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'][:], label='Training loss')\n",
    "plt.plot(history.history['val_loss'][:], label='Validation loss')\n",
    "plt.title('Training and validation loss - MSE')\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction - background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd_prediction = ae.autoencoder(X_test_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction - Beyond Standard Model samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_labels = ['Leptoquark','A to 4 leptons', 'hChToTauNu', 'hToTauTau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_results = []\n",
    "\n",
    "for i, label in enumerate(bsm_labels):\n",
    "    bsm_prediction = ae.autoencoder(bsm_data[i])\n",
    "    \n",
    "    bsm_results.append([label, bsm_target[i], bsm_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = '/eos/user/e/epuljak/forDelphes/CorrectDataResults/AE_result_notpruned.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(output_result, 'w')\n",
    "h5f.create_dataset('QCD_input', data=X_test_flatten)\n",
    "h5f.create_dataset('QCD_target', data=X_test_scaled)\n",
    "h5f.create_dataset('predicted_QCD', data = qcd_prediction)\n",
    "for i, bsm in enumerate(bsm_results):\n",
    "    h5f.create_dataset('%s_scaled' %bsm[0], data=bsm[1])\n",
    "    h5f.create_dataset('%s_input' %bsm[0], data=bsm_data[i])\n",
    "    h5f.create_dataset('predicted_%s' %bsm[0], data=bsm[2])\n",
    "\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "neptune": {
   "notebookId": "db31bc2a-7b92-40f5-8253-dfc65381a6fb",
   "projectVersion": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
